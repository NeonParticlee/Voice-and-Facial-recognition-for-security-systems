{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:57:30.187608Z",
     "start_time": "2024-12-01T16:57:30.184069Z"
    },
    "id": "7tR3vNuhpy6o"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:57:30.420477Z",
     "start_time": "2024-12-01T16:57:30.198152Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PklR2SFxp-_e",
    "outputId": "fbfde1ce-0467-43c6-e61c-f53734a66c32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGGVox(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(1, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=9728, out_features=4096, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VGGVox(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGVox, self).__init__()\n",
    "        # Define the VGGVox architecture (simplified for brevity)\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 96, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2),\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2)\n",
    "        )\n",
    "        # Dynamically calculate the input size for the first fully connected layer\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self._get_conv_output_size((1, 40, 312)), 4096), # Calculate size dynamically based on input shape (1, 40, 312) from your mel spectrogram\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 1024)\n",
    "        )\n",
    "\n",
    "    def _get_conv_output_size(self, input_shape):\n",
    "\n",
    "        \"\"\"\n",
    "        Calculates the output size of the convolutional layers for a given input shape.\n",
    "        This is necessary to dynamically adjust the input size of the first fully connected layer.\n",
    "        \"\"\"\n",
    "        # Create a dummy input tensor with the specified shape\n",
    "        dummy_input = torch.zeros(1, *input_shape)\n",
    "\n",
    "        # Pass the dummy input through the convolutional layers\n",
    "        output = self.conv_layers(dummy_input)\n",
    "\n",
    "        # Calculate the total number of features in the output\n",
    "        output_size = output.view(output.size(0), -1).shape[1]\n",
    "        return output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = VGGVox()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:57:30.433743Z",
     "start_time": "2024-12-01T16:57:30.430810Z"
    },
    "id": "Vt7cBrqgF_H9"
   },
   "outputs": [],
   "source": [
    "audio_file1 = \"D:\\\\voices\\\\Mohammed\\\\M96.wav\"  # Replace with path to the stored user's audio\n",
    "audio_file2 = \"D:\\\\voices\\\\Hussin\\\\H11.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:57:30.504771Z",
     "start_time": "2024-12-01T16:57:30.444504Z"
    },
    "id": "rfCFmzEN_isN"
   },
   "outputs": [],
   "source": [
    "def preprocess_audio(file_path):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(file_path, sr=16000)  # 16kHz sampling rate\n",
    "    # Generate Mel-spectrogram\n",
    "    # The change is on this line: explicitly provide 'y' as a keyword argument\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=40, fmax=8000)\n",
    "    # Convert to log scale\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return log_mel_spec\n",
    "\n",
    "\n",
    "# Cosine Similarity Matching Function\n",
    "def is_match_embedding(embedding1, embedding2, threshold=0.5):\n",
    "    embedding1 = embedding1.cpu().numpy().flatten()\n",
    "    embedding2 = embedding2.cpu().numpy().flatten()\n",
    "    similarity = 1 - cosine(embedding1, embedding2)\n",
    "    print(f\"Embedding Similarity: {similarity:.7f}\")\n",
    "    return similarity >= threshold\n",
    "\n",
    "\n",
    "def get_spectral_centroid(audio_file):\n",
    "    y, sr = librosa.load(audio_file, sr=16000)\n",
    "    cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    avg_cent = np.mean(cent)\n",
    "    return avg_cent\n",
    "\n",
    "\n",
    "def is_match_frequency(audio_file1, audio_file2, threshold=100):\n",
    "    cent1 = get_spectral_centroid(audio_file1)\n",
    "    cent2 = get_spectral_centroid(audio_file2)\n",
    "    diff = np.abs(cent1 - cent2)\n",
    "    print(f\"Spectral Centroid Difference: {diff:.7f}\")\n",
    "    return diff <= threshold\n",
    "\n",
    "\n",
    "# Preprocess and extract embedding for stored user\n",
    "mel_spec1 = preprocess_audio(audio_file1)\n",
    "input_tensor1 = torch.tensor(np.expand_dims(mel_spec1, axis=0), dtype=torch.float32).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    stored_embedding = model(input_tensor1)\n",
    "\n",
    "# Preprocess and extract embedding for new input\n",
    "mel_spec2 = preprocess_audio(audio_file2)\n",
    "input_tensor2 = torch.tensor(np.expand_dims(mel_spec2, axis=0), dtype=torch.float32).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    new_embedding = model(input_tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:57:30.570461Z",
     "start_time": "2024-12-01T16:57:30.516478Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YaRbEaTdS2dw",
    "outputId": "8d4a1383-f22c-452e-dc67-9253f72a3d00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Similarity: 0.9865106\n",
      "Spectral Centroid Difference: 167.9639614\n"
     ]
    }
   ],
   "source": [
    "embedding_match = is_match_embedding(stored_embedding, new_embedding)\n",
    "frequency_match = is_match_frequency(audio_file1, audio_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:57:30.672972Z",
     "start_time": "2024-12-01T16:57:30.667188Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cun3ryMwS6rz",
    "outputId": "2f7f439a-2aa0-4f23-d9d7-21f45794ca7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access Denied: Mismatch in either embedding or frequency.\n"
     ]
    }
   ],
   "source": [
    "if ((embedding_match > 0.5)  and (frequency_match < 200.00)):\n",
    "    print(\"Access Granted: Both embedding and frequency match.\")\n",
    "else:\n",
    "    print(\"Access Denied: Mismatch in either embedding or frequency.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
